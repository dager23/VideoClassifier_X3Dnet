{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash\n",
    "import torch\n",
    "from flash.core.data.utils import download_data\n",
    "from flash.video import VideoClassificationData, VideoClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import kornia.augmentation as K\n",
    "def normalize(x: Tensor) -> Tensor:\n",
    "    return x / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    UniformCropVideo,\n",
    ")\n",
    "from torchvision.transforms import Compose, CenterCrop\n",
    "from torchvision.transforms import RandomCrop\n",
    "from flash.core.data.io.input import DataKeys\n",
    "from flash.core.data.io.input_transform import InputTransform\n",
    "from flash.core.data.transforms import ApplyToKeys\n",
    "from typing import Callable\n",
    "from flash.core.utilities.imports import (\n",
    "    _KORNIA_AVAILABLE,\n",
    "    _PYTORCHVIDEO_AVAILABLE,\n",
    "    requires,\n",
    ")\n",
    "class TransformDataModule(InputTransform):\n",
    "    image_size: int = 256\n",
    "    temporal_sub_sample: int = 16  # This is the only change in our custom transform\n",
    "    mean: Tensor = torch.tensor([0.45, 0.45, 0.45])\n",
    "    std: Tensor = torch.tensor([0.225, 0.225, 0.225])\n",
    "    data_format: str = \"BCTHW\"\n",
    "    same_on_frame: bool = False\n",
    "\n",
    "    def per_sample_transform(self) -> Callable:\n",
    "        per_sample_transform = [CenterCrop(self.image_size)]\n",
    "\n",
    "        return Compose(\n",
    "            [\n",
    "                ApplyToKeys(\n",
    "                    DataKeys.INPUT,\n",
    "                    Compose(\n",
    "                        [UniformTemporalSubsample(self.temporal_sub_sample), normalize]\n",
    "                        + per_sample_transform\n",
    "                    ),\n",
    "                ),\n",
    "                ApplyToKeys(DataKeys.TARGET, torch.as_tensor),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def train_per_sample_transform(self) -> Callable:\n",
    "        per_sample_transform = [RandomCrop(self.image_size, pad_if_needed=True)]\n",
    "\n",
    "        return Compose(\n",
    "            [\n",
    "                ApplyToKeys(\n",
    "                    DataKeys.INPUT,\n",
    "                    Compose(\n",
    "                        [UniformTemporalSubsample(self.temporal_sub_sample), normalize]\n",
    "                        + per_sample_transform\n",
    "                    ),\n",
    "                ),\n",
    "                ApplyToKeys(DataKeys.TARGET, torch.as_tensor),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def per_batch_transform_on_device(self) -> Callable:\n",
    "        return ApplyToKeys(\n",
    "            DataKeys.INPUT,\n",
    "            K.VideoSequential(\n",
    "                K.Normalize(self.mean, self.std),\n",
    "                data_format=self.data_format,\n",
    "                same_on_frame=self.same_on_frame,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\Lib\\site-packages\\pytorch_lightning\\utilities\\apply_func.py:31: LightningDeprecationWarning: `pytorch_lightning.utilities.apply_func.apply_to_collection` has been deprecated in v1.8.0 and will be removed in v2.0.0. Please use `lightning_utilities.core.apply_func.apply_to_collection` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "datamodule = VideoClassificationData.from_folders(\n",
    "    train_folder=\"pen_dataset/train\",\n",
    "    val_folder=\"pen_dataset/val\",\n",
    "    clip_sampler=\"uniform\",\n",
    "    clip_duration=3,\n",
    "    decode_audio=False,\n",
    "    batch_size=1,\n",
    "    transform=TransformDataModule(),  # The custom transform is given to the datamodule's transform argument\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\Lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['metrics'])`.\n",
      "  rank_zero_warn(\n",
      "Using 'x3d_xs' provided by Facebook Research/PyTorchVideo (https://github.com/facebookresearch/pytorchvideo).\n"
     ]
    }
   ],
   "source": [
    "model = VideoClassifier(backbone=\"x3d_xs\", labels=datamodule.labels, pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type       | Params\n",
      "---------------------------------------------\n",
      "0 | train_metrics | ModuleDict | 0     \n",
      "1 | val_metrics   | ModuleDict | 0     \n",
      "2 | test_metrics  | ModuleDict | 0     \n",
      "3 | backbone      | Net        | 3.8 M \n",
      "4 | head          | Sequential | 802   \n",
      "---------------------------------------------\n",
      "32.2 K    Trainable params\n",
      "3.8 M     Non-trainable params\n",
      "3.8 M     Total params\n",
      "15.180    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a25fefad594736aacd3e02670102ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "e:\\anaconda\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de64e02f38f14aaba1dcf989a4c2dbd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e16ccffb32f4ad8acec55ac61e7a99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2bc29e2089b4e02a37abaa59ce95f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d411ffe3597142a39fd2ca44fde96984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = flash.Trainer(max_epochs=3,accelerator=\"gpu\", devices=1,)\n",
    "trainer.finetune(model, datamodule=datamodule, strategy=\"freeze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81c27f46a9f4f57bea0de8dbf40adf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 22it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['unsrcew_back'], ['unsrcew_back']]\n"
     ]
    }
   ],
   "source": [
    "datamodule = VideoClassificationData.from_folders(predict_folder=\"predict\", batch_size=1)\n",
    "predictions = trainer.predict(model, datamodule=datamodule, output=\"labels\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"video_classification.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
